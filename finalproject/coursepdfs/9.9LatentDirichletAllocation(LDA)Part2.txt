Latent Dirichlet Allocation (LDA) Latent Dirichlet Allocation (LDA)  ChengXiang Cheng Zhai Department of Computer Science University of Illinois at Urbana-Champaign  1   Latent Dirichlet Allocation (LDA)  5. Text-based prediction 5. Text-based prediction  3. Topic mining and analysis 3. Topic mining & analysis  Real World  Observed World  Text Data  Perceive  (Perspective)  Express  (English)  1. Natural language 1. Natural language processing and text processing  & text representation representation  4. Opinion mining and sentiment analysis  2. Word association mining and analysis  2  Extensions of PLSA  PLSA with prior knowledge  User-controlled PLSA PLSA as a generative model  Latent Dirichlet Allocation  3   PLSA with Prior Knowledge  Users may have expectations about which topics to analyze:  We expect to see retrieval models as a topic in IR We want to see aspects such as battery and memory for opinions about a laptop  Users may have knowledge about what topics are (or are NOT) covered in a document  Tags = topics  A doc can only be generated using topics corresponding to the tags assigned to the document  We can incorporate such knowledge as priors of PLSA model  4  Maximum a Posteriori (MAP) Estimate  We may use p() to encode all kinds of preferences and constraints, e.g.,  p()>0 if and only if one topic is precisely background: p(w|B)  p()>0 if and only if for a particular doc d, d,3=0 and d,1=1/2 p() favors a  with topics that assign high probabilities to some particular words  The MAP estimate (with conjugate prior) can be computed using a similar EM algorithm to the ML estimate with smoothing to reflect prior preferences  5  )|()(maxarg*Datapp   EM Algorithm with Conjugate Prior on p(w| i)  Prior: p(w|j)  battery 0.5 life  0.5  Pseudo counts of w from prior  +p(w|j) +  What if =0? What if =+?  Sum of all pseudo counts  We may also set any parameter to a constant (including 0) as needed  6  VwCdwdwdCdwdwdjnjVwwdwdVwwdwdnjdkjjnnjdBBBBBwdkjjnnjdjnnjdwdjzpBzpdwcjzpBzpdwcwpjzpBzpdwcjzpBzpdwcwpwpwpBzpwpwpjzp'',',,,)1(',,,,)1(,1)()(,,1'')()(',)()(,,)())(1)(,'()())(1)(,()|()'())(1)(,()())(1)(,()|()1()|()|()()|()|()(Deficiency of PLSA  Not a generative model  Cant compute probability of a new document Heuristic workaround is possible, though  Many parameters  high complexity of models  Many local maxima Prone to overfitting  Not necessarily a problem for text mining (only interested in fitting the training documents)  7   Latent Dirichlet Allocation (LDA)  Make PLSA a generative model by imposing a Dirichlet prior on the model parameters  LDA = Bayesian version of PLSA Parameters are regularized  Can achieve the same goal as PLSA for text mining purposes  Topic coverage and topic word distributions can be inferred using Bayesian inference  8  8   PLSA  LDA  w  p(w|1)  Topic  1  government 0.3 response  0.2 ...  p(w|2)  Topic 2    p(w|k)  Topic k  city 0.2 new   0.1 orleans 0.05 ...  donate  0.1 relief 0.05 help 0.02 ...  Both word distributions and topic choices are free in PLSA  p(1)=d,1  p(2)=d,2    p(k)=d,k  LDA imposes a prior on both  9  ),...,(k,d1,dd))|w(p),...,|w(p(iMi1i)(Dirichlet)(pd0),,...,(ik1)(Dirichlet)(pi0),,...,(iM1   PLSA  LDA  Likelihood Functions for PLSA vs. LDA  Core assumption in all topic models  PLSA component  Added by LDA  10  kjkjCdjddkjjjdVwjkjjjdjdjdddpdpCpdpwpdwcdpwpwp...)|(}){,|(log),|(log)|(])|([log),(}){,|(log)|(}){},{|(111,1,,CdjdjjdjkjjjdVwjdjkjjjdjdjddpCpwpdwcdpwpwp}){},{|(log}){},{|(log])|([log),(}){},{|(log)|(}){},{|(,,1,,1,,Parameter Estimation and Inferences in LDA  Parameters can be estimated using ML estimator  How many parameters in LDA vs. PLSA?  However, {j} and {d,j} must now be computed using posterior inference  Computationally intractable Must resort to approximate inference Many different inference methods are available  11  ),|(logmaxarg),(,Cp     Summary of Probabilistic Topic Models  Probabilistic topic models provide a general principled way of mining and analyzing topics in text with many applications Basic task setup:  Input: Text data Output: k topics + proportions of these topics covered in each document  PLSA is the basic topic model, often adequate for most applications LDA improves over PLSA by imposing priors  Theoretically more appealing Practically, LDA and PLSA perform similarly for many tasks  12  Suggested Readings  Blei, D. 2012. Probabilistic Topic Models. Communications of the ACM 55 (4): 7784. doi: 10.1145/2133806.2133826. Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai. Automatic Labeling of Multinomial Topic Models. Proceedings of ACM KDD 2007, pp. 490-499, DOI=10.1145/1281192.1281246. Yue Lu, Qiaozhu Mei, and Chengxiang Zhai. 2011. Investigating task performance of probabilistic topic models: an empirical study of PLSA and LDA. Information Retrieval, 14, 2 (April 2011), 178-203. DOI=10.1007/s10791-010-9141-9.  13 
