Evaluation of TR Systems: Practical Issues Evaluation of TR Systems: Practical Issues  ChengXiang Cheng  Zhai Department of Computer Science University of Illinois at Urbana-Champaign  1   2  Challenges in Creating a Test Collection  Queries: representative & many  Relevance Judgments  Existence of relevant docs  Q1  Q2 Q3 Q50 ...  D2  D1 D3 D48  Docs: representative & many  Measures: capture the perceived utility by users  Judgments: completeness vs. minimum human work  Q1  D1  + Q1  D2  + Q1  D3 Q1  D4 Q1  D5 +  Q2  D1 Q2  D2 + Q2  D3 + Q2  D4  Q50 D1 Q50 D2 Q50 D3 +   3   Statistical Significance Tests  How sure can you be that an observed difference doesnt simply result from the particular queries you chose?  Experiment 1 System A System B  Query  Experiment 2 System A System B  Query  1 2 3 4 5 6 7  0.20 0.21 0.22 0.19 0.17 0.20 0.21  Average 0.20  Slide from Doug Oard & Philip Resnik  0.40 0.41 0.42 0.39 0.37 0.40 0.41  0.40  1 2 3 4 5 6 7  0.02 0.39 0.16 0.58 0.04 0.09 0.12  Average 0.20  0.76 0.07 0.37 0.21 0.02 0.91 0.46  0.40  4  Statistical Significance Testing  Query  System A  System B  Sign Test  Wilcoxon  1 2 3 4 5 6 7  0.02 0.39 0.16 0.58 0.04 0.09 0.12  Average 0.20  0.76 0.07 0.37 0.21 0.02 0.91 0.46  0.40  + - + - - + +  +0.74 - 0.32 +0.21 - 0.37 - 0.02 +0.82 +0.34  p=1.0  p=0.9375  95% of outcomes  Slide from Doug Oard & Philip Resnik Try some out at: http://www.fon.hum.uva.nl/Service/CGI-Inline/HTML/Statistics.html  0  5  Pooling: Avoid Judging all Documents  If we cant afford judging all the documents in the collection, which subset should we judge? Pooling strategy  Choose a diverse set of ranking methods (TR systems) Have each to return top-K documents Combine all the top-K sets to form a pool for human assessors to judge Other (unjudged) documents are usually assumed to be non-relevant (though they dont have to) Okay for comparing systems that contributed to the pool, but problematic for evaluating new systems  6  Summary of TR Evaluation  Extremely important!  TR is an empirically defined problem Inappropriate experiment design misguides research and applications Make sure to get it right for your research or application  Cranfield evaluation methodology is the main paradigm  MAP and nDCG: appropriate for comparing ranking algorithms Precision@10docs is easier to interpret from a user s perspective  Not covered  A-B Test  [Sanderson 10] User studies  [Kelly 09]  7  Additional Readings  Donna Harman, Information Retrieval Evaluation. Synthesis Lectures on Information Concepts, Retrieval, and Services, Morgan & Claypool Publishers 2011  Mark Sanderson, Test Collection Based Evaluation of Information Retrieval Systems. Foundations and Trends in Information Retrieval 4(4): 247-375 (2010)  Diane Kelly,  Methods for Evaluating Interactive Information Retrieval Systems with Users. Foundations and Trends in Information Retrieval 3(1-2): 1-224 (2009)  8 
