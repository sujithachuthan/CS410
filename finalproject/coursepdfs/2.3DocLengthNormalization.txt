Text Retrieval and Search Engines Text Retrieval and Search Engines  Vector Space Model: Doc Length Normalization  ChengXiang Cheng  Zhai Department of Computer Science University of Illinois at Urbana-Champaign  ChengXiang Cheng Zhai  Department of Computer Science University of Illinois at Urbana-Champaign  1    Course Schedule  Small Relevant Data  User  11. Recommendation  2. Text Access  Recommender System  Search Engine  1. Natural Language Content Analysis  Big Text Data  3. Text Retrieval Problem  4. Text Retrieval Methods  5. Vector Space Model  6. System Implementation  7. Evaluation  8. Probabilistic Model  9. Feedback  10. Web Search  2  What about Document Length?  Query = news about presidential campaign  d4  news of presidential campaign presidential candidate  100 words  d6 > d4?  d6  5000 words  campaign...... campaign................................................... . ...........news.................................................................................  news..   presidential . presidential  Document Length Normalization  Penalize a long doc with a doc length normalizer  Long doc has a better chance to match any query Need to avoid over-penalization  A document is long because  it uses more words  more penalization it has more contents  less penalization  Pivoted length normalizer: average doc length as pivot  Normalizer = 1 if |d| =average doc length (avdl)  4     Pivoted Length Normalization  Reward  1.0  b>>0 b>0  b=0  Penalization  0      1      2                 avdl  |d|  Shorter than avdl  Longer than avdl  5  avdldbbnormalizer||1]1,0[bState of the Art VSM Ranking Functions  Pivoted Length Normalization VSM [Singhal et al 96]  BM25/Okapi [Robertson & Walker 94]  6  dqwwdfMavdldbbkdwcdwckqwcdqf)(1log)||1(),(),()1(),(),(dqwwdfMavdldbbdwcqwcdqf)(1log||1)]],(1ln[1ln[),(),(),0[,]1,0[31kkb   Further Improvement of VSM?  Improved instantiation of dimension?  stemmed words, stop word removal, phrases, latent semantic indexing (word clusters), character n-grams, bag-of-words with phrases is often sufficient in practice Language-specific and domain-specific tokenization is important to ensure normalization of terms  Improved  instantiation of similarity function?  cosine of angle between two vectors? Euclidean? dot product seems still the best (sufficiently general especially with appropriate term weighting)  7   Further Improvement of BM25  BM25F [Robertson & Zaragoza 09]  Use BM25 for documents with structures (F=fields) Key idea: combine the frequency counts of terms in all fields and then apply BM25 (instead of the other way)  BM25+ [Lv & Zhai 11]  Address the problem of over penalization of long documents by BM25 by adding a small constant to TF Empirically and analytically shown to be better than BM25  8  Summary of Vector Space Model  Relevance(q,d) = similarity(q,d) Query and documents are represented as vectors Heuristic design of ranking function Major term weighting heuristics  TF weighting and transformation IDF weighting Document length normalization  BM25 and Pivoted normalization seem to be most effective  9   Additional Readings  A. Singhal, C. Buckley, and M. Mitra. Pivoted document length normalization. In Proceedings of ACM SIGIR 1996. S. E. Robertson and S. Walker. Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval, Proceedings of ACM SIGIR 1994. S. Robertson and H. Zaragoza. The Probabilistic Relevance Framework: BM25 and Beyond, Found. Trends Inf. Retr. 3, 4 (April 2009). Y. Lv, C. Zhai, Lower-bounding term frequency normalization. In Proceedings of ACM CIKM 2011.  10 
