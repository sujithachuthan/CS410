Probabilistic Retrieval Model: Probabilistic Retrieval Model: Statistical Language Model  ChengXiang Cheng  Zhai Department of Computer Science University of Illinois at Urbana-Champaign  1   Probabilistic Retrieval Model: Statistical Language Model  Overview  What is a Language Model? Unigram Language Model Uses of a Language Model  3  What is a Statistical Language Model (LM)?  A probability distribution over word sequences  p(Today is Wednesday)  0.001  p(Today Wednesday is)  0.0000000000001  p(The eigenvalue is positive)  0.00001  Context-dependent! Can also be regarded as a probabilistic mechanism for generating  text, thus also called a generative model  Today is Wednesday  Today Wednesday is    The eigenvalue is positive  4   Why is a LM Useful?  Quantify the uncertainties in natural language  Allows us to answer questions like:  Given that we see John and feels, how likely will we see happy as opposed to habit as the next word? (speech recognition) Given that we observe baseball three times and game once in a news article, how likely is it about sports?         (text categorization, information retrieval) Given that a user is interested in sports news, how likely would the user use baseball in a query?  (information retrieval)  5   The Simplest Language Model: Unigram LM  Generate text by generating each word INDEPENDENTLY Thus, p(w1 w2 ... wn)=p(w1)p(w2)p(wn) Parameters: {p(wi)}  p(w1)++p(wN)=1 (N is voc. size) Text = sample drawn according to this word distribution  Wednesday  today   eigenvalue  p(today is Wed) = p(today)p(is)p(Wed) = 0.0002  0.001  0.000015  6  Text Generation with Unigram LM  Unigram LM  p(w|)  Sampling  Document =?  Topic 1: Text mining   text  0.2 mining 0.1 association 0.01 clustering 0.02  food 0.00001   Topic 2: Health   food 0.25 nutrition 0.1 healthy 0.05 diet 0.02   Text mining paper  Food nutrition paper  7   Estimation of Unigram LM  Unigram LM  p(w|)=?  Estimation  10/100 5/100 3/100 3/100  1/100   text  ? mining ? association ? database ?  query ?   Maximum Likelihood (ML) Estimator:  Text Mining Paper  d  Total #words=100 text 10 mining 5 association 3 database 3 algorithm 2  query 1 efficient 1  Is this the best estimate?  8  ||),()|()|(ddwcdwpwp     LMs for Topic Representation  General Background English Text  B  Computer Science Papers  C  Text mining paper  d  the 0.03 a 0.02 is 0.015 we 0.01 ... food 0.003 computer 0.00001 text  0.000006   the 0.032 a 0.019 is 0.014 we 0.011 ... computer 0.004 software 0.0001 text  0.00006   the 0.031  text  0.04 mining 0.035 association 0.03 clustering 0.005 computer 0.0009  food 0.000001  Collection LM: p(w|C) Document LM: p(w|d) Background LM: p(w|B)  9  LMs for Association Analysis  What words are semantically related to computer?  Documents containing word  computer  Topic LM: p(w|computer) the 0.032 a 0.019 is 0.014 we 0.008 computer 0.004 software 0.0001  Background LM: p(w|B)  B  General Background English Text  the 0.03 a 0.02 is 0.015 we 0.01 ... computer 0.00001  Normalized Topic LM:  p(w|computer)/p(w|B)  computer 400 software 150 program 104  text  3.0  the 1.1 a 0.99 is 0.9 we 0.8  10  Summary  Language Model = probability distribution over text Unigram Language Model = word distribution Uses of a Language Model  Representing topics Discovering word associations  11  Additional Readings  Chris Manning and Hinrich Schtze, Foundations of Statistical Natural Language Processing,  MIT Press. Cambridge, MA: May 1999. Rosenfeld, R., "Two decades of statistical language modeling: where do we go from here?," Proceedings of the IEEE , vol.88, no.8, pp.1270,1278, Aug. 2000  12 
