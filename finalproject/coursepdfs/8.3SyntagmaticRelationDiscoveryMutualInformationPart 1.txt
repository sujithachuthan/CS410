Syntagmatic Relation Discovery: Syntagmatic Relation Discovery: Mutual Information  ChengXiang Cheng Zhai Department of Computer Science University of Illinois at Urbana-Champaign  1   Syntagmatic Relation Discovery: Mutual Information  5. Text-based prediction 5. Text-based prediction  3. Topic mining and analysis 3. Topic mining & analysis  Real World  Observed World  Text Data  Perceive  (Perspective)  Express  (English)  1. Natural language 1. Natural language processing and text processing  & text representation representation  4. Opinion mining and sentiment analysis  2. Word association mining and analysis  2  Mutual Information I(X;Y): Measuring Entropy Reduction  How much reduction in the entropy of X can we obtain by knowing Y?  Mutual Information:  I(X; Y)= H(X)  H(X|Y) = H(Y)-H(Y|X)  Properties: - Non-negative:  I(X;Y)0 - Symmetric:  I(X;Y)=I(Y;X) - I(X;Y)=0  iff X & Y are independent  When we fix X to rank different Ys, I(X;Y) and H(X|Y) give the same order but I(X;Y) allows us to compare different (X,Y) pairs.  3  Mutual Information I(X;Y) for Syntagmatic Relation Mining  Mutual Information:  I(X; Y)= H(X)  H(X|Y) = H(Y)-H(Y|X)  Whenever eats occurs, what other words also tend to occur?  Which words have high mutual information with eats?  I(Xeats; Xmeats) = I(Xmeats; Xeats)  >  I(Xeats; Xthe) = I(Xthe; Xeats)  I(Xeats; Xeats) =H(Xeats)  I(Xeats; Xw)  4    Rewriting Mutual Information (MI) Using KL-divergence  The observed joint distribution of XW1 and XW2  The expected joint distribution of XW1 and XW2 if XW1 and XW2 were independent  MI measures the divergence of the actual joint distribution from the expected distribution under the independence assumption. The larger the divergence is, the higher the MI would be.  5  )vX(p)uX(p)vX,uX(plog)vX,uX(p)X;X(I2w1w2w1w22w}1,0{u}1,0{v1w2w1wProbabilities Involved in Mutual Information  Presence & absence of w1: Presence & absence of w2:  p(XW1=1) + p(XW1=0) = 1 p(XW2=1) + p(XW2=0) = 1  Co-occurrences of w1 and w2: p(XW1=1, XW2=1) + p(XW1=1, XW2=0)+p(XW1=0, XW2=1) + p(XW1=0, XW2=0) = 1  Both w1 & w2 occur  Only w1 occurs Only w2 occurs  None of them occurs  6  )vX(p)uX(p)vX,uX(plog)vX,uX(p)X;X(I2w1w2w1w22w}1,0{u}1,0{v1w2w1wRelations Between Different Probabilities  Presence & absence of w1: Presence & absence of w2:  p(XW1=1) + p(XW1=0) = 1 p(XW2=1) + p(XW2=0) = 1  Co-occurrences of w1 and w2: p(XW1=1, XW2=1) + p(XW1=1, XW2=0)+p(XW1=0, XW2=1) + p(XW1=0, XW2=0) = 1  Constraints: p(XW1=1, XW2=1) + p(XW1=1, XW2=0) = p(XW1=1)  p(XW1=0, XW2=1) + p(XW1=0, XW2=0)= p(XW1=0)  p(XW1=1, XW2=1) + p(XW1=0, XW2=1) = p(XW2=1)  p(XW1=1, XW2=0) + p(XW1=0, XW2=0) = p(XW2=0)  7  Computation of Mutual Information  Presence & absence of w1:  p(XW1=1) + p(XW1=0) = 1  Presence & absence of w2: Co-occurrences of w1 and w2: p(XW1=1, XW2=1) + p(XW1=1, XW2=0)+p(XW1=0, XW2=1) + p(XW1=0, XW2=0) = 1  p(XW2=1) + p(XW2=0) = 1  p(XW1=1, XW2=1) + p(XW1=1, XW2=0) =  p(XW1=1)  p(XW1=0, XW2=1) + p(XW1=0, XW2=0)= p(XW1=0)  p(XW1=1, XW2=1) + p(XW1=0, XW2=1) = p(XW2=1)  p(XW1=1, XW2=0) + p(XW1=0, XW2=0) = p(XW2=0)  We only need to know p(XW1=1), p(XW2=1), and p(XW1=1, XW2=1).  8  Estimation of Probabilities (Depending on the Data)  W1      W2  Segment_1        1       0 Segment_2        1       1 Segment_3        1       1  Only W1 occurred  Both occurred  Both occurred  Segment_4        0       0  Neither occurred    Segment_N        0        1  Only W2 occurred  Count(w1) = total number segments that contain W1 Count(w2) = total number segments that contain W2 Count(w1, w2) = total number segments that contain both W1 and W2  9  N)2w,1w(count)1X,1X(pN)2w(count)1X(pN)1w(count)1X(p2w1w2w1wSmoothing: Accommodating Zero Counts  W1      W2  PseudoSeg_1        0           0 PseudoSeg_2        1           0 PseudoSeg_3        0           1  PseudoSeg_4        1           1  Segment_1           1          0    Segment_N          0          1  Smoothing: Add pseudo data so that no event has zero counts (pretend we observed extra data)  Actually observed data  10  1N25.0)2w,1w(count)1X,1X(p1N5.0)2w(count)1X(p1N5.0)1w(count)1X(p2w1w2w1wSummary of Syntagmatic Relation Discovery  Syntagmatic relation can be discovered by measuring correlations between occurrences of two words. Three concepts from Information Theory:  Entropy H(X): measures the uncertainty of a random variable X Conditional entropy H(X|Y): entropy of X given we know Y Mutual information I(X;Y): entropy reduction of X (or Y) due to knowing Y (or X)  Mutual information provides a principled way for discovering syntagmatic relations.  11   Summary of Word Association Mining  Two basic associations: paradigmatic and syntagmatic  Generally applicable to any items in any language (e.g., phrases or entities as units)  Pure statistical approaches are available for discovering both (can be combined to perform joint analysis).  Generally applicable to any text with no human effort Different ways to define context and segment lead to interesting variations of applications  Discovered associations can support many other applications.  12   Additional Reading  Chris Manning and Hinrich Schtze, Foundations of Statistical Natural Language Processing, MIT Press. Cambridge, MA: May 1999. (Chapter 5 on collocations) Chengxiang Zhai, Exploiting context to identify lexical atoms: A statistical view of linguistic context. Proceedings of the International and Interdisciplinary Conference on Modelling and Using Context (CONTEXT-97), Rio de Janeiro, Brzil, Feb. 4-6, 1997. pp. 119-129. Shan Jiang and ChengXiang Zhai, Random walks on adjacency graphs for mining lexical relations from big text data. Proceedings of IEEE BigData Conference 2014, pp. 549-554.  13 
